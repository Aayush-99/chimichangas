{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotblib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import world\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Initial states of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 20\n",
    "ACTION_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_state(state, maxlen):\n",
    "    if len(state) > maxlen:\n",
    "        return state[:maxlen]\n",
    "    elif len(state) < maxlen:\n",
    "        new_state = np.zeros((maxlen,))\n",
    "        new_state[:len(state)] = state\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(players, my_particles, killed):\n",
    "    global STATE_SIZE\n",
    "    \n",
    "    initial_state = []\n",
    "    for i in range(len(players)):\n",
    "        if i not in killed:\n",
    "            env_particles,env_particle_distance = food_in_env(players[i], my_particles)\n",
    "            env_food_vector = getFoodVector(players[i],env_particles, my_particles)\n",
    "            env_food_vector = sum(env_food_vector, [])\n",
    "\n",
    "            env_players, env_player_distance = players_in_env(players[i],players)\n",
    "            env_player_vector = getPlayerVector(players[i],env_players, players)\n",
    "            env_player_vector = sum(env_player_vector, [])\n",
    "\n",
    "            temp_state = [env_food_vector, env_player_vector]\n",
    "            temp_state = sum(temp_state, [])\n",
    "            initial_state.append(np.array(temp_state))\n",
    "        else:\n",
    "            initial_state.append(np.array([0]))\n",
    "\n",
    "    initial_state = [pad_state(state, STATE_SIZE) for state in initial_state]\n",
    "\n",
    "    return np.array(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=STATE_SIZE, h_size=16, a_size=ACTION_SIZE):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATION: 0 , score: [-339, -44]\n",
      "GENERATION: 1 , score: [-518, -257]\n",
      "GENERATION: 2 , score: [-859, -299]\n"
     ]
    }
   ],
   "source": [
    "policy_1 = Policy().to(device)\n",
    "policy_2 = Policy().to(device)\n",
    "agents = [policy_1, policy_2]\n",
    "optimizers = [optim.Adam(policy.parameters(), lr=1e-2) for policy in agents]\n",
    "\n",
    "\n",
    "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    \n",
    "    TIME = 0\n",
    "    regenerate_times = 0\n",
    "    MAX_REGENERATIONS = 100\n",
    "    allow_regenerate = True\n",
    "\n",
    "    \n",
    "    players, killed, my_particles = world.init()\n",
    "    \n",
    "    states = get_state(players, my_particles, killed)\n",
    "\n",
    "    scores = [0 for _ in range(len(players))]\n",
    "    saved_log_probs = {i:[] for i in range(len(players))}\n",
    "    rewards = {i:[] for i in range(len(players))}\n",
    "    \n",
    "    while True:\n",
    "        for i, agent in enumerate(agents):\n",
    "            if i not in killed:\n",
    "                action, log_prob = agents[i].act(states[i])\n",
    "                saved_log_probs[i].append(log_prob)\n",
    "                reward, done, players, my_particles, killed, TIME = world.take_action(players, my_particles, killed, i, action, TIME)\n",
    "                next_states = get_state(players, my_particles, killed)\n",
    "                rewards[i].append(reward)\n",
    "                scores[i] += reward\n",
    "                states = next_states\n",
    "            \n",
    "        if(len(killed) == len(players) and allow_regenerate):\n",
    "            discounts = {j:[gamma**i for i in range(len(rewards[j])+1)] for j in range(len(players))}\n",
    "            R = {j:sum([a*b for a,b in zip(discounts[j], rewards[j])]) for j in range(len(players))}\n",
    "            \n",
    "            policy_loss = {i:[] for i in range(len(players))}\n",
    "            for i, saved_log_prob in saved_log_probs.items():\n",
    "                for log_prob in saved_log_prob:\n",
    "                    policy_loss[i].append(-log_prob * R[i])\n",
    "                policy_loss[i] = torch.cat(policy_loss[i]).sum()\n",
    "            \n",
    "                optimizers[i].zero_grad()\n",
    "                policy_loss[i].backward(retain_graph=True)\n",
    "                optimizers[i].step()\n",
    "            \n",
    "            killed = []\n",
    "            players = regenerate_species(TIME)\n",
    "            print(\"GENERATION:\", regenerate_times, \", score:\", scores)\n",
    "            regenerate_times += 1\n",
    "        elif(len(killed) == INITIAL_POPULATION and not allow_regenerate):\n",
    "            running = False\n",
    "\n",
    "        if(regenerate_times == MAX_REGENERATIONS):\n",
    "            allow_regenerate = False\n",
    "            break\n",
    "        \n",
    "        \n",
    "#         if i_episode % print_every == 0:\n",
    "#             print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "#         if np.mean(scores_deque)>=200.0:\n",
    "#             print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "#             break\n",
    "        \n",
    "#     return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
