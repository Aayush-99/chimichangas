{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chimichangas Agent Training\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import world\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial states of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 20\n",
    "ACTION_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_state(state, maxlen):\n",
    "    if len(state) > maxlen:\n",
    "        return state[:maxlen]\n",
    "    elif len(state) < maxlen:\n",
    "        new_state = np.zeros((maxlen,))\n",
    "        new_state[:len(state)] = state\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(players, my_particles, killed):\n",
    "    global STATE_SIZE\n",
    "    \n",
    "    initial_state = []\n",
    "    for i in range(len(players)):\n",
    "        if i not in killed:\n",
    "            env_particles,env_particle_distance = food_in_env(players[i], my_particles)\n",
    "            env_food_vector = getFoodVector(players[i],env_particles, my_particles)\n",
    "            env_food_vector = sum(env_food_vector, [])\n",
    "\n",
    "            env_players, env_player_distance = players_in_env(players[i],players)\n",
    "            env_player_vector = getPlayerVector(players[i],env_players, players)\n",
    "            env_player_vector = sum(env_player_vector, [])\n",
    "\n",
    "            temp_state = [env_food_vector, env_player_vector]\n",
    "            temp_state = sum(temp_state, [])\n",
    "            initial_state.append(np.array(temp_state))\n",
    "        else:\n",
    "            initial_state.append(np.array([0]))\n",
    "\n",
    "    initial_state = [pad_state(state, STATE_SIZE) for state in initial_state]\n",
    "\n",
    "    return np.array(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Multi Agent Deep Deterministic Policy Gradients (MADDPG) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpg_agent import Agent\n",
    "from buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = Agent(state_size=STATE_SIZE, action_size=ACTION_SIZE, random_seed=0)\n",
    "agent_2 = Agent(state_size=STATE_SIZE, action_size=ACTION_SIZE, memory=agent_1.memory, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [agent_1, agent_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=2000, print_every=100):\n",
    "    \n",
    "    TIME = 0\n",
    "    regenerate_times = 0\n",
    "    MAX_REGENERATIONS = 100\n",
    "    allow_regenerate = True\n",
    "\n",
    "    \n",
    "    players, killed, my_particles = world.init()\n",
    "    \n",
    "    states = get_state(players, my_particles, killed)\n",
    "    \n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    add_noise = True\n",
    "    agent_1.reset()\n",
    "    agent_2.reset()\n",
    "    \n",
    "    scores = [0 for _ in range(len(players))]\n",
    "    \n",
    "    while True:\n",
    "        for i, agent in enumerate(agents):\n",
    "            if i not in killed:\n",
    "                action = agent.act(states[i], add_noise=add_noise).tolist()\n",
    "                reward, done, players, my_particles, killed, TIME = world.take_action(players, my_particles, killed, i, action, TIME)\n",
    "                next_states = get_state(players, my_particles, killed)\n",
    "                agent.step(states[i], action, reward, next_states[i], done)\n",
    "                scores[i] += reward\n",
    "                states = next_states\n",
    "            \n",
    "        if(len(killed) == len(players) and allow_regenerate):\n",
    "            killed = []\n",
    "            players = regenerate_species(TIME)\n",
    "            print(\"GENERATION:\", regenerate_times, \", score:\", scores)\n",
    "            regenerate_times += 1\n",
    "        elif(len(killed) == INITIAL_POPULATION and not allow_regenerate):\n",
    "            running = False\n",
    "\n",
    "        if(regenerate_times == MAX_REGENERATIONS):\n",
    "            allow_regenerate = False\n",
    "            break\n",
    "            \n",
    "#         max_score = np.max(scores_ep)\n",
    "#         scores_deque.append(max_score)\n",
    "#         scores.append(max_score)\n",
    "#         print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), \n",
    "#                                                                          max_score), end=\"\")\n",
    "#         if i_episode % print_every == 0:\n",
    "#             print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "#         if np.mean(scores_deque) >= 0.5:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, \n",
    "#                                                                                          np.mean(scores_deque)))\n",
    "#             torch.save(agent_1.actor_local.state_dict(), 'checkpoint_1_actor.pth')\n",
    "#             torch.save(agent_1.critic_local.state_dict(), 'checkpoint_1_critic.pth')\n",
    "#             torch.save(agent_2.actor_local.state_dict(), 'checkpoint_2_actor.pth')\n",
    "#             torch.save(agent_2.critic_local.state_dict(), 'checkpoint_2_critic.pth')\n",
    "#             break\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the average score during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
