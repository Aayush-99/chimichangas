{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import world\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Initial states of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = 21\n",
    "ACTION_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_state(state, maxlen):\n",
    "    if len(state) > maxlen:\n",
    "        return state[:maxlen]\n",
    "    elif len(state) < maxlen:\n",
    "        new_state = np.zeros((maxlen,))\n",
    "        new_state[:len(state)] = state\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(players, my_particles, killed):\n",
    "    global STATE_SIZE\n",
    "    \n",
    "    initial_state = []\n",
    "    for i in range(len(players)):\n",
    "        if i not in killed:\n",
    "            env_particles,env_particle_distance = food_in_env(players[i], my_particles)\n",
    "            env_food_vector = getFoodVector(players[i],env_particles, my_particles)\n",
    "            env_food_vector = sum(env_food_vector, [])\n",
    "\n",
    "            env_players, env_player_distance = players_in_env(players[i],players)\n",
    "            env_player_vector = getPlayerVector(players[i],env_players, players)\n",
    "            env_player_vector = sum(env_player_vector, [])\n",
    "\n",
    "            temp_state = [env_food_vector, env_player_vector, [players[i].energy]]\n",
    "            temp_state = sum(temp_state, [])\n",
    "            initial_state.append(np.array(temp_state))\n",
    "        else:\n",
    "            initial_state.append(np.array([0]))\n",
    "\n",
    "    initial_state = [pad_state(state, STATE_SIZE) for state in initial_state]\n",
    "\n",
    "    return np.array(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, s_size=STATE_SIZE, h_size=16, a_size=ACTION_SIZE):\n",
    "        super(Agent, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "agents = [Agent().to(device) for _ in range(world.INITIAL_POPULATION)]\n",
    "optimizers = [optim.Adam(policy.parameters(), lr=1e-2) for policy in agents]\n",
    "\n",
    "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    \n",
    "    TIME = 0\n",
    "    regenerate_times = 0\n",
    "    MAX_REGENERATIONS = 100\n",
    "    allow_regenerate = True\n",
    "\n",
    "    \n",
    "    players, killed, my_particles = world.init()\n",
    "    \n",
    "    states = get_state(players, my_particles, killed)\n",
    "\n",
    "    scores = [0 for _ in range(len(players))]\n",
    "    saved_log_probs = {i:[] for i in range(len(players))}\n",
    "    rewards = {i:[] for i in range(len(players))}\n",
    "    \n",
    "    while True:\n",
    "        if(len(killed) == len(players)):\n",
    "            print(killed)\n",
    "            break\n",
    "        for i, agent in enumerate(agents):\n",
    "            if i not in killed:\n",
    "                action, log_prob = agents[i].act(states[i])\n",
    "                saved_log_probs[i].append(log_prob)\n",
    "                reward, done, players, my_particles, killed, TIME = world.take_action(players, my_particles, killed, i, action, TIME)\n",
    "                rewards[i].append(reward)\n",
    "                \n",
    "                if(action == 10 and reward == 4):\n",
    "                    print(\"Asexual reproduction\")\n",
    "                    offsprings = len(players) - len(agents)\n",
    "                    for j in range(len(agents), len(agents) + offsprings):\n",
    "                        agents.append(Agent().to(device))\n",
    "                        optimizers.append(optim.Adam(policy.parameters(), lr=1e-2))\n",
    "                        scores.append(0)\n",
    "                        saved_log_probs[j] = []\n",
    "                        rewards[j] = []\n",
    "                elif(action == 10 and reward == 4):\n",
    "                    print(\"Sexual reproduction\")\n",
    "                    dominant_percent = random.randint(0, 10) * 10\n",
    "                    recessive_percent = 100 - dominant_percent\n",
    "                    offsprings = len(players) - len(agents)\n",
    "                    num_dominant = round(offsprings * (dominant_percent / 100))\n",
    "                    num_recessive = offsprings - num_dominant\n",
    "                    \n",
    "                    for j in range(len(agents), len(agents) + num_dominant):\n",
    "                        agents.append(Agent().to(device))\n",
    "                        optimizers.append(optim.Adam(policy.parameters(), lr=1e-2))\n",
    "                        scores.append(0)\n",
    "                        saved_log_probs[j] = []\n",
    "                        rewards[j] = []\n",
    "                    for j in range(len(agents) + num_dominant, len(agents) + num_dominant + num_recessive):\n",
    "                        agents.append(Agent().to(device))\n",
    "                        optimizers.append(optim.Adam(policy.parameters(), lr=1e-2))\n",
    "                        scores.append(0)\n",
    "                        saved_log_probs[j] = []\n",
    "                        rewards[j] = []\n",
    "                \n",
    "                if(i in killed):\n",
    "                    agents[i] = 0\n",
    "                    \n",
    "                optimizers[i].zero_grad()\n",
    "                policy_loss = 0\n",
    "                for j in range(len(saved_log_probs[i])):\n",
    "                    policy_loss += (-saved_log_probs[i][j] * rewards[i][j])\n",
    "                policy_loss.backward(retain_graph=True)\n",
    "                optimizers[i].step()\n",
    "                \n",
    "                next_states = get_state(players, my_particles, killed)\n",
    "                rewards[i].append(reward)\n",
    "                scores[i] += reward\n",
    "                states = next_states\n",
    "            \n",
    "#         if(len(killed) == len(players) and allow_regenerate):\n",
    "#             discounts = {j:[gamma**i for i in range(len(rewards[j])+1)] for j in range(len(players))}\n",
    "#             R = {j:sum([a*b for a,b in zip(discounts[j], rewards[j])]) for j in range(len(players))}\n",
    "            \n",
    "#             policy_loss = {i:[] for i in range(len(players))}\n",
    "#             for i, saved_log_prob in saved_log_probs.items():\n",
    "#                 for log_prob in saved_log_prob:\n",
    "#                     policy_loss[i].append(-log_prob * R[i])\n",
    "#                 policy_loss[i] = torch.cat(policy_loss[i]).sum()\n",
    "            \n",
    "#                 optimizers[i].zero_grad()\n",
    "#                 policy_loss[i].backward(retain_graph=True)\n",
    "#                 optimizers[i].step()\n",
    "            \n",
    "#             killed = []\n",
    "#             players = regenerate_species(TIME)\n",
    "#             print(\"GENERATION:\", regenerate_times, \", score:\", scores)\n",
    "#             regenerate_times += 1\n",
    "#         elif(len(killed) == INITIAL_POPULATION and not allow_regenerate):\n",
    "#             running = False\n",
    "\n",
    "#         if(regenerate_times == MAX_REGENERATIONS):\n",
    "#             allow_regenerate = False\n",
    "#             break\n",
    "        \n",
    "        \n",
    "#         if i_episode % print_every == 0:\n",
    "#             print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "#         if np.mean(scores_deque)>=200.0:\n",
    "#             print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "#             break\n",
    "        \n",
    "#     return scores\n",
    "    \n",
    "scores = reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
